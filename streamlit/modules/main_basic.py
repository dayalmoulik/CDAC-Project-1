# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hSNgFu5tdQuwNWj0MOQGfMP05D1BZNPT

## **Contents of the Workbook :**
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""## **Step - 1 :** *Importing Necesaary Libraries and Datasets*

### **Step - 1.1 :** *Importing Necesaary Packages & Libraries*
"""

# !pip install import_ipynb
# !pip install -q -U "tensorflow-text==2.8.*"

# !pip freeze
# 
# Commented out IPython magic to ensure Python compatibility.
# DataFrame
import pandas as pd

# Matplot
import matplotlib.pyplot as plt
# %matplotlib inline

from matplotlib.ticker import MaxNLocator
import matplotlib.gridspec as gridspec
import matplotlib.patches as mpatches

# Scikit-learn

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.metrics import f1_score, accuracy_score


# Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Bidirectional
from tensorflow.keras import utils
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

# nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import nltk
nltk.download('omw-1.4')

# Word2vec
import gensim
from gensim.test.utils import common_texts
from gensim.models import Word2Vec


# Utility
import string
import re
import numpy as np
import os
import logging
import time
import pickle
import itertools
import random
import datetime

# WordCloud
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from collections import Counter, defaultdict

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Warnings
import warnings 
warnings.filterwarnings('ignore')

# Set log
# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/'

import import_ipynb
import preprocessing_module as preprocessing
import visualization_module as visualization

"""### **Step - 1.2 :** *Importing Dataset*"""

# Varaibles related to dataset
# DATASET_COLUMNS = ["clean_text", "category"]
# DATASET_ENCODING = "ISO-8859-1"
TRAIN_SIZE = 0.8

# Parameters related to KERAS
SEQUENCE_LENGTH = 150
EPOCHS = 10
BATCH_SIZE = 128

# Variables for Exporting purpose
KERAS_MODEL = "model.h5"
WORD2VEC_MODEL = "model.w2v"
TOKENIZER_MODEL = "/content/tokenizer.pkl"
ENCODER_MODEL = "encoder.pkl"

# MISC

plt.style.use('fivethirtyeight')
pd.options.display.max_columns = 250
pd.options.display.max_rows = 250

#abstraction
'''
test_csv_path = '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/files/unlabelled/for_test/test_model.csv'
TOKENIZER_MODEL = "/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/tokenizer.pkl"

model_loaded = tf.keras.models.load_model('/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/final/model.h5')
testing_df = pd.read_csv(test_csv_path)

testing_df = preprocessing.drop_duplicates(testing_df)
testing_df = preprocessing.cleaning(testing_df, prediction=True)
testing_df.head()

with open(TOKENIZER_MODEL, 'rb') as handle:
    tokenizer_loaded = pickle.load(handle)

testing_df_txt2seq = pad_sequences(tokenizer_loaded.texts_to_sequences(testing_df.cleaned_sentence), maxlen=SEQUENCE_LENGTH)

y_pred_score = model_loaded.predict(testing_df_txt2seq)
y_pred_conf_score, y_pred_label = pred_score2label(y_pred_score)

pred_df = output_pred_csv(testing_df, y_pred_conf_score, y_pred_label)
pred_df.head(20)
'''

##preprocessing-starts
csv_folder_path = '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/files/labelled'


df = preprocessing.concat_csvs(csv_folder_path)
df = preprocessing.drop_duplicates(df)
df = preprocessing.drop_na_values_category(df)
df = preprocessing.cleaning(df)

df.head()

#outputting merged cleaned csv
# preprocessing.output_merged_cleaned_csv(df)

print("DataFrame Shape:",df.shape)
df.describe()

#preprocessing-ends

"""### **Step - 2.2 :** Null Values Identification and Treatment

## **Step - 2 :** *Data Pre-processing*

### **Step - 2.3 :** Label Encoding
"""

visualization.plot_category_counts(df['category'])

#temp
# df['sub-category'].value_counts()
# df_sub = preprocessing.drop_na_values_subcategory(df)

# df_sub.head()
# plot_category_counts(df_sub['sub-category'])

"""### **Step - 2.4 :** Train-Test Split"""

df_train, df_test = preprocessing.splitting_train_test(df, TRAIN_SIZE)

encoder, y_train, y_test = preprocessing.labelEncoding_target_variables(df_train, df_test)

#word2vec

w2v_model, w2v_weights, w2v_vocab_size, w2v_embedding_size = preprocessing.create_Word2Vec_embedding(df_train)

print("Vocabulary Size: {} - Embedding Dim: {}".format(w2v_vocab_size, w2v_embedding_size))
print('Word2Vec_weights Shape:',w2v_weights.shape)

#testing similar words from word2vec embeddings
w2v_model.wv.most_similar("minister")

"""### **Step - 5.1 :** Token and Vocab Creation"""

def tokenizing(df_train):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(df_train.cleaned_sentence)

    vocab_size = len(tokenizer.word_index) + 1
    # print("Total words", vocab_size)

    return vocab_size, tokenizer

vocab_size, tokenizer = tokenizing(df_train)

def converting_txt2seq(tokenizer, df_train, df_test, SEQUENCE_LENGTH):
    x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.cleaned_sentence), maxlen=SEQUENCE_LENGTH)
    x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.cleaned_sentence), maxlen=SEQUENCE_LENGTH)

    return x_train, x_test

x_train, x_test = preprocessing.converting_txt2seq(tokenizer, df_train, df_test, SEQUENCE_LENGTH)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

"""## STEP 5.1.2 Visualizing Word2Vec Embeddings with t-SNE"""

plot_n_words = 100
# visualization.plot_word2vec_embedding_by_tsne(w2v_model, w2v_vocab_size, plot_n_words)

"""### **Step - 5.3 :** Embedding Layer Creation"""

W2V_SIZE = 50
embedding_matrix = preprocessing.create_embeddingMatrix_from_Word2Vec(w2v_model, vocab_size, W2V_SIZE, tokenizer)

model_loaded = tf.keras.models.load_model('/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/final/model.h5')

model_loaded.summary()



"""### **Step - 5.4 :** Model Creation - LSTM"""

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=w2v_embedding_size,
                    weights=[embedding_matrix],
                    input_length=SEQUENCE_LENGTH,
                    mask_zero=True,
                    trainable=False))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))

model.summary()

"""### **Step - 5.5 :** Compiling Model"""

model.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

"""### **Step - 5.6 :** Callback Creation"""

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]

"""### **Step - 5.7 :** Model Training """

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(x_train, y_train,
#                     batch_size=32,
#                     epochs=2,
#                     validation_split=0.1,
#                     verbose=1
#                     )

"""### **Step - 5.8 :** Model Evaluation """

# Commented out IPython magic to ensure Python compatibility.
# %%time
# score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
# print()
# print("ACCURACY:",score[1])
# print("LOSS:",score[0])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
 
epochs = range(len(acc))
 
plt.plot(epochs, acc, label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
 
plt.figure()
 
plt.plot(epochs, loss, label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
 
plt.show()

hist_df = pd.DataFrame(history.history)
visualization.plot_model_performance_graph(hist_df)

def pred_score2label(y_pred_score):
    y_pred_conf_score,y_pred_num , y_pred_label = [], [], []
    for i in y_pred_score:
        if i<=0.5:
            pred_label = 0
            pred_score = i + 0.5
        else:
            pred_label = 1
            pred_score = i

        y_pred_num.append(pred_label)
        y_pred_conf_score.append(pred_score)

        pred_li_temp = [pred_label]
        y_pred_temp =  list(encoder.inverse_transform(pred_li_temp) )

        y_pred_label.append(y_pred_temp)
    return y_pred_conf_score, y_pred_num, y_pred_label

y_pred_score = model_loaded.predict(x_test)
y_pred_conf_score,y_pred_num, y_pred_label = pred_score2label(y_pred_score)


    # print(type(i))

# (y_pred_num[:4], y_pred_label[:4])
y_test = [i[0] for i in y_test]
y_test[:4], y_pred_conf_score[:4]

# x_test = [i[0] for i in x_test]

(x_test)

def output_pred_csv(df_test, y_pred_conf_score, y_pred_label):

    sent = df_test.cleaned_sentence.tolist()
    y_pred_conf_score = [i[0] for i in y_pred_conf_score]
    y_pred_label = [i[0] for i in y_pred_label]

    pred_df = pd.DataFrame(list(zip(sent, y_pred_conf_score, y_pred_label)), columns =['Sentence', 'confidence_score', 'y_pred_label'])
    # pred_df.to_csv(r'/content/pred_df.csv', index=False)

    return pred_df

output_pred_csv(df_test, y_pred_conf_score, y_pred_label)



c_r = classification_report(y_test, y_pred_num) 

visualization.plot_classification_report(c_r)

confusion_matrix(y_test, y_pred_conf_score)

from sklearn.metrics import plot_confusion_matrix

plot_confusion_matrix(model_loaded, y_test, y_pred_conf_score)  
plt.show()

"""### **Step - 5.9 :** Prediction using Model """

predict("I will kill you")

predict("I have nothing to lose")

predict("Experience has been bad")

"""### **Step - 5.10 :** Creating Confusion Matrix """

# Commented out IPython magic to ensure Python compatibility.
# %%time
# y_pred_1d = []
# y_test_1d = list(df_test.category)
# scores = model.predict(x_test, verbose=1, batch_size=32)
# y_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]

def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """

    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=30)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)
    plt.yticks(tick_marks, classes, fontsize=22)

    fmt = '.2f'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label', fontsize=25)
    plt.xlabel('Predicted label', fontsize=25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)
# plt.figure(figsize=(12,12))
# plot_confusion_matrix(cnf_matrix, classes=df_train.category.unique(), title="Confusion matrix")
# plt.show()

"""### **Step - 5.11 :** Classification Report """

print(classification_report(y_test_1d, y_pred_1d))

"""### **Step - 5.12 :** Accuracy Score """

accuracy_score(y_test_1d, y_pred_1d)

"""### **Step - 5.13 :** Saving Model for future use """

model_path = '/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/model.h5'
TOKENIZER_MODEL = "/content/drive/MyDrive/CDAC-SELF/CDAC_PROJECT/Mental_Graph_Of_User_Using_Social_Media_Posts/project_code/src/model/tokenizer.pkl"


model.save(model_path)
# w2v_model.save(WORD2VEC_MODEL)
# pickle.dump(tokenizer, open(TOKENIZER_MODEL, "wb"), protocol=0)
# pickle.dump(encoder, open(ENCODER_MODEL, "wb"), protocol=0)

"""## **References:**

* **LSTM Overview** : [**Link**](https://www.analyticsvidhya.com/blog/2022/03/an-overview-on-long-short-term-memory-lstm/)
* **Code Optimisation Techniques** : [**Link**](https://www.geeksforgeeks.org/optimization-tips-python-code/)
"""